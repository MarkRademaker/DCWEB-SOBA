{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisualizationT-SNE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkRademaker/DCWEB-SOBA/blob/main/VisualizationT_SNE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqd2SNv7JVYQ"
      },
      "source": [
        "import pprint\n",
        "import pandas as pd\n",
        "import json\n",
        "words = []\n",
        "vectors = []\n",
        "sentenceIDs = []\n",
        "\n",
        "file = open('file.txt', 'r')\n",
        "total = file.read()\n",
        "total = total[1:-2]\n",
        "wordAr = total.split('}, \"') \n",
        "counter = 0\n",
        "for ar in wordAr:\n",
        "    counter+=1\n",
        "    arr = ar.split('{\"word\": \"')\n",
        "    arr1 = arr[1].split('\", \"vector\": [')\n",
        "    arr2 = arr1[1].split(', \"sentence id\": ')\n",
        "    \n",
        "    stringg = arr1[0]\n",
        "    vecs = arr2[0].split('],[')\n",
        "    sentenceID = int(arr2[1])+1\n",
        "    for v in vecs:\n",
        "        words.append(stringg)\n",
        "        list1 = list(v.split(','))\n",
        "        veclist = []\n",
        "        for item in list1:\n",
        "            if item[-1] == ']':\n",
        "                item = item[:-1]\n",
        "            if item[-2:] == ']]':\n",
        "                item = item[:-2]\n",
        "            if item[-3:] == ']]}':\n",
        "                item = item[:-3]\n",
        "            veclist.append(float(item))\n",
        "        vectors.append(veclist)\n",
        "    sentenceIDs.append(sentenceID)\n",
        "print(\"A list of the clustered vectors is made with a list with corresponding labels\")\n",
        "print(\"Number of words contained in the Json: \", counter)\n",
        "print(\"Number of vectors contained in the Json: \", len(vectors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpfSdsmfJVYQ"
      },
      "source": [
        "#Makes lists of vectors necessary for analysis \n",
        "listGenPosWords=[\n",
        "    'good','decent','great','tasty','fantastic','solid','yummy','terrific'\n",
        "]\n",
        "\n",
        "listGenNegWords=[\n",
        "    'bad','awful','horrible','terrible','poor','shitty','horrid'\n",
        "]\n",
        "\n",
        "listGenPosWordsWrong=[\n",
        "    'fantastic','great','terrific', \n",
        "    'good','great','awesome',\n",
        "    'nice','excellent',\n",
        "    'right','wonderful',              \n",
        "    'enjoyable','enjoy','love','perfectly',\n",
        "    'perfect','happy','Fascinating',\n",
        "    'appreciated','amazing','delightful','surprising',\n",
        "    'fine'\n",
        "]\n",
        "listGenNegWordsWrong=[\n",
        "    'horrible','bad','hate','unpleasant',\n",
        "    'lousy','awful','worst','terrible',\n",
        "    'poorly','poor','crappy','underwhelming',\n",
        "    'overvalued', 'unsatisfactory'\n",
        "]\n",
        "\n",
        "listType2PosWords = [\n",
        "    'Classy','Cozy','Warm','Lively',\n",
        "    'Beautiful','Charming','Famous',\n",
        "    'Popular','admirable','inviting','Quick',\n",
        "    'Rapid','Better','Gentle','Generous',\n",
        "    'Friendly','Welcoming','Admirable','inviting','Fast',\n",
        "    'Culinary','Innovative','Sustainable',\n",
        "    'Affordable','Payable','fair','Colorful','Garnished',\n",
        "    'Delicious','Fresh','Addictive',\n",
        "    'Finger-licking','Renowned','Masterful',\n",
        "    'Must-have','Nutritious','tasty','Organic','Juicy','Recommend',\n",
        "    'Recommended','Payable','Inexpensive',\n",
        "    'Fair','Delicious',\n",
        "    'Fresh','Appetizing','Exotic',\n",
        "    'Refreshing','yummy','Mouthwatering'\n",
        "]\n",
        "listType2NegWords = [\n",
        "    'Cold','Silent','disturbing','boring',\n",
        "    'Ugly','Miserable','crowded','Hard','Slow','Unfriendly','Lousy','unreasonable',\n",
        "    'Cheap', 'Expensive','high', 'Disgusting','Gross','Spoiled',\n",
        "    'Underripe','Overcooked','Undercooked','Greasy','Unhealthy',\n",
        "    'Overpriced','Pricy','Plain','Canned'\n",
        "]\n",
        "listType3Words = ['bad','good','great','horrible','beautiful','love','hate','nice','rude','gorgeous','awful','decent','fantastic']\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmlGLgqdJVYR"
      },
      "source": [
        "genericpos=[]\n",
        "genericneg=[]\n",
        "for index,word in enumerate(words):\n",
        "    for pos in listGenPosWords:\n",
        "        if word == pos.lower():\n",
        "            #genericpos.append(string1)\n",
        "            genericpos.append([word,vectors[index]])\n",
        "            break\n",
        "    for neg in listGenNegWords:\n",
        "        if word == neg.lower():\n",
        "            #genericneg.append(string1)\n",
        "            genericneg.append([word,vectors[index]])\n",
        "            break\n",
        "            \n",
        "type2PosWords = []\n",
        "type2NegWords = []\n",
        "for index,word in enumerate(words):\n",
        "    for pos in listType2PosWords:\n",
        "        if word == pos.lower():\n",
        "            #genericpos.append(string1)\n",
        "            type2PosWords.append([word,vectors[index]])\n",
        "            break\n",
        "    for neg in listType2NegWords:\n",
        "        if word == neg.lower():\n",
        "            #genericneg.append(string1)\n",
        "            type2NegWords.append([word,vectors[index]])\n",
        "            break\n",
        "  type3Words = []\n",
        "for index,word in enumerate(words):\n",
        "    for pos in listType3Words:\n",
        "        if word == pos.lower():\n",
        "            #genericpos.append(string1)\n",
        "            type3Words.append([word,vectors[index],sentenceIDs[index]])\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS0u7p8YkzD8"
      },
      "source": [
        "#### import important methods used\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RVgXF8PJVYS"
      },
      "source": [
        "\n",
        "# FOR GENERIC -> TYPE 1\n",
        "wordsForLab = []\n",
        "vectors1 = []\n",
        "for i in genericpos:\n",
        "    wordsForLab.append(i[0])\n",
        "    vectors1.append(i[1])\n",
        "numpos = len(vectors1)\n",
        "for i in genericneg:\n",
        "    wordsForLab.append(i[0])\n",
        "    vectors1.append(i[1])\n",
        "labels = []\n",
        "i = 0\n",
        "while i<len(genericpos):\n",
        "    labels.append('green')\n",
        "    i+=1\n",
        "j = 0\n",
        "while j<len(genericneg):\n",
        "    labels.append('red')\n",
        "    j+=1\n",
        "print(len(vectors1),numpos,len(vectors1)-numpos)\n",
        "\n",
        "# FOR ASPECT SPECIFIC -> TYPE 2\n",
        "wordsForLab2 = []\n",
        "vectors2= []\n",
        "labels2 = []\n",
        "for i in type2Words:\n",
        "  name =  i[0]# + ',' + str(i[2])\n",
        "  wordsForLab2.append(name)\n",
        "  vectors2.append(i[1])\n",
        "\n",
        "  #ASSIGNING LABELS TO SPECIFIC WORDS\n",
        "  if i[2] < 6:\n",
        "      labels3.append('green')\n",
        "  elif i[2] >5 :\n",
        "      labels3.append('red') \n",
        "j = 0\n",
        "print(labels2)\n",
        "print(len(wordsForLab2))\n",
        "# FOR ASPECT SPECIFIC -> TYPE 3\n",
        "wordsForLab3 = []\n",
        "vectors3= []\n",
        "labels3 = []\n",
        "for i in type3Words:\n",
        "  name =  i[0]# + ',' + str(i[2])\n",
        "  wordsForLab3.append(name)\n",
        "  vectors3.append(i[1])\n",
        "\n",
        "  #ASSIGNING LABELS TO SPECIFIC WORDS\n",
        "  if i[2] < 6:\n",
        "      labels3.append('green')\n",
        "  elif i[2] >5 :\n",
        "      labels3.append('red') \n",
        "j = 0\n",
        "print(labels3)\n",
        "print(len(wordsForLab3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wrxuhFWZJVYT"
      },
      "source": [
        "#Make visualisation of word embeddings using t-SNE\n",
        "\n",
        "tsne = TSNE(n_components=2,perplexity=4,n_iter=100000,random_state=42)\n",
        "x_5_1 = tsne.fit_transform(vectors3)\n",
        "#x_6_1 = tsne.fit_transform(vectors1)\n",
        "plt.scatter(x_5_1[:,0], x_5_1[:,1], color=labels3)\n",
        "#plt.scatter(x_6_1[:,0], x_6_1[:,1], color=labels)\n",
        "\n",
        "for i, txt in enumerate(wordsForLab3):\n",
        "    plt.annotate(txt, (x_5_1[i,0], x_5_1[i,1]))\n",
        "#for i, txt in enumerate(wordsForLab):\n",
        "    #plt.annotate(txt, (x_6_1[i,0], x_6_1[i,1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}