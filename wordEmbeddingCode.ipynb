{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install methods\n",
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"Belangrijke libraries zijn geimporteerd.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data for creating word embeddings\n",
    "\n",
    "i=0\n",
    "reviews=[]\n",
    "file = open('Desktop/1restDataSome.txt', 'r')\n",
    "review = file.read()\n",
    "review = review.replace(\"\\n\", ' ')\n",
    "\n",
    "#Reviews are split on ,|,\n",
    "wrong_review = review.split(\",|,\") \n",
    "for w in wrong_review:\n",
    "    i += 1\n",
    "    reviews.append(w)\n",
    "print(\"Number of reviews : \",len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for removing negation words in same sentence\n",
    "i = 0\n",
    "review = 0\n",
    "sentC=0\n",
    "reviewsNew = []\n",
    "neg = \" not \"\n",
    "neg2 = \" nothing \"\n",
    "neg3 = \" never \"\n",
    "neg4 = \" didn\\'t \"\n",
    "neg5 = \" wouldn\\'t \"\n",
    "neg6 = \" don\\'t \"\n",
    "neg7 = \" can\\'t \"\n",
    "neg8 = \" doesn\\'t \"\n",
    "neg9 = \" couldn\\'t \"\n",
    "case=0\n",
    "for z in reviews:\n",
    "    sent2New = []\n",
    "    sentNew= \"\"\n",
    "    sent = z.split('.')\n",
    "    case = 0\n",
    "    for j in sent:\n",
    "        sent2 = j.split('!')\n",
    "        for l in sent2:\n",
    "            sentC+=1\n",
    "            if (neg in l) or (neg2 in l) or (neg3 in l) or (neg4 in l) or (neg5 in l) or (neg6 in l) or (neg7 in l) or (neg8 in l) or (neg9 in l):\n",
    "                i += 1\n",
    "                sent2.remove(l)\n",
    "                if case == 0:\n",
    "                    review += 1\n",
    "                    case = 1\n",
    "        sent2New.append(\"!\".join(sent2))\n",
    "    #print(sent2New)\n",
    "    sentNew = (\".\".join(sent2New))\n",
    "    reviewsNew.append(str(sentNew))\n",
    "\n",
    "print(\"Number of sentences with negation word:\", i)\n",
    "print(\"Number of reviews with these negation word:\" ,review)\n",
    "print(\"Number of sentences in text: \", sentC)\n",
    "print(\"Number of sentences in text after removing sentences: \", sentC - i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model (weights)\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True,)                    #pretrained\n",
    "#model = BertModel.from_pretrained('/Users/markrademaker/Desktop/PostBERTModel100K',output_hidden_states = True,)                    #posttrained\n",
    "#model = BertModel.from_pretrained('/Users/markrademaker/Desktop/BERT_FineTuned_Final',output_hidden_states = True,)                    #posttrained\n",
    "#model = BertForSequenceClassification.from_pretrained('Desktop/SavedModel',output_hidden_states = True,) #finetuned\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "print(\"BERT model is gedownload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pretrained vectors\n",
    "start = time.time()\n",
    "bert_vectors = []\n",
    "review_counter = 0\n",
    "#loops over the reviews\n",
    "for rev in reviews:\n",
    "    result = []\n",
    "    tokenized_text = tokenizer.tokenize(rev)\n",
    "    #change to 512 or shorter\n",
    "    if len(tokenized_text)>=512:\n",
    "        del tokenized_text[512:len(tokenized_text)]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #get all results for one review and display\n",
    "    # replace with whole vector!\n",
    "    i=0\n",
    "    while i < len(tokenized_text):\n",
    "        veccie = [round(vec,4) for vec in token_vecs_sum[i].tolist()]\n",
    "        result.append([tokenized_text[i],indexed_tokens[i],segments_ids[i], veccie, review_counter])\n",
    "        i += 1\n",
    "    bert_vectors.append(result)\n",
    "    del result\n",
    "    review_counter+=1\n",
    "end = time.time()\n",
    "print(\"Tijd: \", end-start, \"s\")\n",
    "print(\"Reviews zijn getokenized en omgezet naar vectoren!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights for fine-tuned model here to create word embeddings with fine-tuned model\n",
    "model = BertModel.from_pretrained('Desktop/BERT_FineTuned_Final',output_hidden_states = True,)                    #posttrained\n",
    "model.eval()\n",
    "print(\"BERT Fine-Tuned model is gedownload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates finetuned vectors\n",
    "start = time.time()\n",
    "bert_vectors_ft = []\n",
    "review_counter = 0\n",
    "#loops over the reviews\n",
    "for rev in reviews:\n",
    "    result = []\n",
    "    tokenized_text = tokenizer.tokenize(rev)\n",
    "    #change to 512 or shorter\n",
    "    if len(tokenized_text)>=512:\n",
    "        del tokenized_text[512:len(tokenized_text)]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #get all results for one review and display\n",
    "    # replace with whole vector!\n",
    "    i=0\n",
    "    while i < len(tokenized_text):\n",
    "        veccie = [round(vec,4) for vec in token_vecs_sum[i].tolist()]\n",
    "        result.append([tokenized_text[i],indexed_tokens[i],segments_ids[i], veccie, review_counter])\n",
    "        i += 1\n",
    "    bert_vectors_ft.append(result)\n",
    "    del result\n",
    "    review_counter+=1\n",
    "end = time.time()\n",
    "print(\"Tijd: \", end-start, \"s\")\n",
    "print(\"Reviews zijn getokenized en omgezet naar vectoren!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create good format for ontology\n",
    "#Change list1 to the location of bert vectors\n",
    "start = time.time()\n",
    "list1 = bert_vectors_ft\n",
    "words = {}\n",
    "j=1\n",
    "counterRev = 0\n",
    "for rev in list1:\n",
    "    counterRev += 1\n",
    "    for word in rev:\n",
    "        string1 = str(word[0])\n",
    "        words[j] = {'word': string1,\n",
    "                    'vector': word[3]\n",
    "                    ,'sentence id': word[4]\n",
    "                   }\n",
    "        j = j+1\n",
    "        \n",
    "print('Number of reviews done: ', counterRev, ', with ', j, ' words in total.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file\n",
    "import json\n",
    "start = time.time()\n",
    "name_file = \"file.txt\"\n",
    "with open(name_file, 'w') as outfile:\n",
    "    json.dump(words, outfile)\n",
    "end = time.time()\n",
    "print(\"Tijd: \", end-start, \"s\")\n",
    "print(\"Alles staat in \" + name_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
