{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./opt/anaconda3/lib/python3.8/site-packages (4.3.3)\n",
      "Requirement already satisfied: sacremoses in ./opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: filelock in ./opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in ./opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./opt/anaconda3/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: packaging in ./opt/anaconda3/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in ./opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: six in ./opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in ./opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Belangrijke libraries zijn geimporteerd.\n"
     ]
    }
   ],
   "source": [
    "#install methods\n",
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"Belangrijke libraries zijn geimporteerd.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews :  14\n"
     ]
    }
   ],
   "source": [
    "#load data for creating word embeddings\n",
    "\n",
    "i=0\n",
    "reviews=[]\n",
    "file = open('Desktop/test_example.txt', 'r')\n",
    "review = file.read()\n",
    "review = review.replace(\"\\n\", ' ')\n",
    "\n",
    "#Reviews are split on ,|,\n",
    "wrong_review = review.split(\",|,\") \n",
    "for w in wrong_review:\n",
    "    i += 1\n",
    "    reviews.append(w)\n",
    "print(\"Number of reviews : \",len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for removing negation words in same sentence\n",
    "i = 0\n",
    "review = 0\n",
    "sentC=0\n",
    "reviewsNew = []\n",
    "neg = \" not \"\n",
    "neg2 = \" nothing \"\n",
    "neg3 = \"never \"\n",
    "neg4 = \" didn\\'t\"\n",
    "neg5 = \" wouldn\\'t\"\n",
    "neg6 = \" don\\'t\"\n",
    "neg7 = \" can\\'t\"\n",
    "neg8 = \" doesn\\'t\"\n",
    "neg9 = \" coudn't\"\n",
    "case=0\n",
    "for z in reviews:\n",
    "    sent2New = []\n",
    "    sentNew= \"\"\n",
    "    sent = z.split('.')\n",
    "    case = 0\n",
    "    for j in sent:\n",
    "        sent2 = j.split('!')\n",
    "        for l in sent2:\n",
    "            sentC+=1\n",
    "            if (neg in l) or (neg2 in l) or (neg3 in l) or (neg4 in l) or (neg5 in l) or (neg6 in l) or (neg7 in l) or (neg8 in l) or (neg9 in l):\n",
    "                i += 1\n",
    "                sent2.remove(l)\n",
    "                if case == 0:\n",
    "                    review += 1\n",
    "                    case = 1\n",
    "        sent2New.append(\"!\".join(sent2))\n",
    "    #print(sent2New)\n",
    "    sentNew = (\".\".join(sent2New))\n",
    "    reviewsNew.append(str(sentNew))\n",
    "\n",
    "print(\"Number of sentences with negation word:\", i)\n",
    "print(\"Number of reviews with these negation word:\" ,review)\n",
    "print(\"Number of sentences in text: \", sentC)\n",
    "print(\"Number of sentences in text after removing sentences: \", sentC - i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model is gedownload.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True,)                    #pretrained\n",
    "#model = BertModel.from_pretrained('/Users/markrademaker/Desktop/PostBERTModel100K',output_hidden_states = True,)                    #posttrained\n",
    "#model = BertModel.from_pretrained('/Users/markrademaker/Desktop/BERT_FineTuned_Final',output_hidden_states = True,)                    #posttrained\n",
    "#model = BertForSequenceClassification.from_pretrained('Desktop/SavedModel',output_hidden_states = True,) #finetuned\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "print(\"BERT model is gedownload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1015, 1010, 1013, 1010, 1045, 2097, 3942, 4977, 2279, 3204, 2006, 1037,\n",
      "         2082, 4440]])\n",
      "tensor([[1016, 1010, 1013, 1010, 2017, 2323, 2175, 1998, 3113, 2026, 2767, 2002,\n",
      "         2003, 2013, 4977]])\n",
      "tensor([[1017, 1010, 1013, 1010, 2748, 1996, 2156, 2379, 4977, 2003, 2200, 4010]])\n",
      "tensor([[1018, 1010, 1013, 1010, 5483, 2003, 1996, 2406, 2279, 2000, 4977]])\n",
      "tensor([[1019, 1010, 1013, 1010, 1996, 2374, 2136, 1997, 4977, 2003, 2652, 3492,\n",
      "         2092, 2012, 1996, 2088, 2452]])\n",
      "tensor([[ 1020,  1010,  1013,  1010,  1996,  2231,  1997,  4977,  2003,  3492,\n",
      "         13593]])\n",
      "tensor([[ 1021,  1010,  1013,  1010,  2748,  1010,  2043,  2017,  2202,  1996,\n",
      "          4977,  2041,  1997,  1996, 17428,  3531,  4521,  2009,  2007,  1037,\n",
      "          5442,  1998,  9292]])\n",
      "tensor([[1022, 1010, 1013, 1010, 1045, 1005, 1040, 9544, 2000, 4521, 4977, 2612,\n",
      "         1997, 7975, 2005, 4234]])\n",
      "tensor([[1023, 1010, 1013, 1010, 2043, 1996, 4977, 2003, 2145, 6315, 1010, 2017,\n",
      "         2323, 5525, 2709, 2009, 2000, 1996, 3829]])\n",
      "tensor([[2184, 1010, 1013, 1010, 2026, 2905, 6732, 4977, 2003, 7977, 1010, 1045,\n",
      "         2293, 2009, 2043, 2026, 3566, 3084, 2009]])\n",
      "tensor([[ 2340,  1010,  1013,  1010,  1996,  2087, 12090,  2833,  2005,  4343,\n",
      "          1997,  2251,  2003,  4977]])\n",
      "tensor([[2260, 1010, 1013, 1010, 3531, 2064, 2057, 2031, 4977, 2005, 4596, 3892]])\n",
      "tensor([[2410, 1010, 1013, 1010, 2065, 1045, 2018, 2000, 5454, 1045, 2052, 2215,\n",
      "         2000, 3942, 3304, 2279, 2095]])\n",
      "tensor([[ 2403,  1010,  1013,  1010,  2748,  2006,  5958,  1045,  1005,  1040,\n",
      "          2066,  2000,  4521,  1037, 10733,  2043,  3666,  2694,  1012]])\n",
      "Tijd:  1.178041696548462 s\n",
      "Reviews zijn getokenized en omgezet naar vectoren!\n"
     ]
    }
   ],
   "source": [
    "#Create pretrained vectors\n",
    "start = time.time()\n",
    "bert_vectors = []\n",
    "review_counter = 0\n",
    "#loops over the reviews\n",
    "for rev in reviews:\n",
    "    result = []\n",
    "    tokenized_text = tokenizer.tokenize(rev)\n",
    "    #change to 512 or shorter\n",
    "    if len(tokenized_text)>=512:\n",
    "        del tokenized_text[512:len(tokenized_text)]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    print(tokens_tensor)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #get all results for one review and display\n",
    "    # replace with whole vector!\n",
    "    i=0\n",
    "    while i < len(tokenized_text):\n",
    "        veccie = [round(vec,4) for vec in token_vecs_sum[i].tolist()]\n",
    "        result.append([tokenized_text[i],indexed_tokens[i],segments_ids[i], veccie, review_counter])\n",
    "        i += 1\n",
    "    bert_vectors.append(result)\n",
    "    del result\n",
    "    review_counter+=1\n",
    "end = time.time()\n",
    "print(\"Tijd: \", end-start, \"s\")\n",
    "print(\"Reviews zijn getokenized en omgezet naar vectoren!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Fine-Tuned model is gedownload.\n"
     ]
    }
   ],
   "source": [
    "#Load weights for fine-tuned model here to create word embeddings with fine-tuned model\n",
    "model = BertModel.from_pretrained('Desktop/fine_tune_no_post',output_hidden_states = True,)                    #posttrained\n",
    "model.eval()\n",
    "print(\"BERT Fine-Tuned model is gedownload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tijd:  1.1343469619750977 s\n",
      "Reviews zijn getokenized en omgezet naar vectoren!\n"
     ]
    }
   ],
   "source": [
    "#Creates finetuned vectors\n",
    "start = time.time()\n",
    "bert_vectors = []\n",
    "review_counter = 0\n",
    "#loops over the reviews\n",
    "for rev in reviews:\n",
    "    result = []\n",
    "    tokenized_text = tokenizer.tokenize(rev)\n",
    "    #change to 512 or shorter\n",
    "    if len(tokenized_text)>=512:\n",
    "        del tokenized_text[512:len(tokenized_text)]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    if len(tokenized_text) == 0 :\n",
    "        continue\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #get all results for one review and display\n",
    "    # replace with whole vector!\n",
    "    i=0\n",
    "    while i < len(tokenized_text):\n",
    "        veccie = [round(vec,4) for vec in token_vecs_sum[i].tolist()]\n",
    "        result.append([tokenized_text[i],indexed_tokens[i],segments_ids[i], veccie, review_counter])\n",
    "        i += 1\n",
    "    bert_vectors.append(result)\n",
    "    del result\n",
    "    review_counter+=1\n",
    "end = time.time()\n",
    "print(\"Tijd: \", end-start, \"s\")\n",
    "print(\"Reviews zijn getokenized en omgezet naar vectoren!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews done:  14 , with  220  words in total.\n"
     ]
    }
   ],
   "source": [
    "#Create good format for ontology\n",
    "#Change list1 to the location of bert vectors\n",
    "start = time.time()\n",
    "list1 = bert_vectors\n",
    "words = {}\n",
    "j=1\n",
    "counterRev = 0\n",
    "for rev in list1:\n",
    "    counterRev += 1\n",
    "    for word in rev:\n",
    "        string1 = str(word[0])\n",
    "        words[j] = {'word': string1,\n",
    "                    'vector': word[3]\n",
    "                    ,'sentence id': word[4]\n",
    "                   }\n",
    "        j = j+1\n",
    "        \n",
    "print('Number of reviews done: ', counterRev, ', with ', j, ' words in total.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tijd:  0.19252300262451172 s\n",
      "Alles staat in Daviddd.txt\n"
     ]
    }
   ],
   "source": [
    "#Save file\n",
    "import json\n",
    "start = time.time()\n",
    "name_file = \"Daviddd.txt\"\n",
    "with open(name_file, 'w') as outfile:\n",
    "    json.dump(words, outfile)\n",
    "end = time.time()\n",
    "print(\"Tijd: \", end-start, \"s\")\n",
    "print(\"Alles staat in \" + name_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
