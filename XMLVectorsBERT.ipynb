{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentences from xml file \n",
    "import os\n",
    "from xml.etree import ElementTree\n",
    "file_name = 'file'\n",
    "full_file = os.path.join(file_name)\n",
    "\n",
    "dom = ElementTree.parse(full_file)\n",
    "\n",
    "sentences = dom.findall('/Review/sentences/sentence/text')\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant models\n",
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"Belangrijke libraries zijn geimporteerd.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model (weights)\n",
    "#\n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True,)                    #pretrained\n",
    "#model = BertModel.from_pretrained('/Users/markrademaker/Desktop/PostBERTModel100K',output_hidden_states = True,)                    #posttrained\n",
    "#model = BertModel.from_pretrained('/Users/markrademaker/Desktop/BERT_FineTuned_Final',output_hidden_states = True,)                    #posttrained\n",
    "#model = BertForSequenceClassification.from_pretrained('Desktop/SavedModel',output_hidden_states = True,) #finetuned\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "print(\"BERT model is gedownload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceList = []\n",
    "for s in sentences:\n",
    "    sentenceList.append(s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "bert_vectors = []\n",
    "sentence_counter = 0\n",
    "#loops over the reviews\n",
    "for sen in sentenceList:\n",
    "    result = []\n",
    "    tokenized_text = tokenizer.tokenize(sen)\n",
    "    #change to 512 or shorter\n",
    "    if len(tokenized_text)>=512:\n",
    "        del tokenized_text[512:len(tokenized_text)]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #get all results for one review and display\n",
    "    # replace with whole vector!\n",
    "    i=0\n",
    "    while i < len(tokenized_text):\n",
    "        veccie = [round(vec,4) for vec in token_vecs_sum[i].tolist()]\n",
    "        result.append([tokenized_text[i], veccie])\n",
    "        i += 1\n",
    "    bert_vectors.append(result)\n",
    "    del result\n",
    "    sentence_counter+=1\n",
    "end = time.time()\n",
    "print(\"Tijd: \", end-start, \"s\")\n",
    "print(\"Sentences zijn getokenized en omgezet naar vectoren!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write in correct format for HAABSA\n",
    "senCount = 0\n",
    "bertVecString =''\n",
    "outF = open(\"myOutFileTEST.txt\",\"w\")\n",
    "for i in bert_vectors:\n",
    "    senCount+=1\n",
    "    for j in i:\n",
    "        bertVecString =''\n",
    "        word = str(j[0])\n",
    "        vector = str(j[1])\n",
    "        vector = vector.replace(\",\", '')\n",
    "        vector = vector.replace(\"[\",'')\n",
    "        vector = vector.replace(']','')\n",
    "        bertVecString += word + \" \"\n",
    "        bertVecString += vector\n",
    "        outF.write(bertVecString)\n",
    "        outF.write(\"\\n\")\n",
    "outF.close()\n",
    "print(\"All\", senCount,\" sentences now in\" , outF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
